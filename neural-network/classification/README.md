# Classificação de picos de consumo:

Este projeto tem como objetivo prever picos de consumo de CPU baseado na relação entre as varíaveis das 60 primeiras ocorrências (1 hora) com os picos das próximas 60 ocorrências.

### O ciclo de treino

<img src="https://github.com/vinhali/advanced_monitoring/blob/master/neural-network/classification/img/estrutura.png?raw=true"/>

### Formúla para cada item

<img src="https://github.com/vinhali/advanced_monitoring/blob/master/neural-network/classification/img/mat.png?raw=true"/>

### Rectified Linear Unit

<img src="https://github.com/vinhali/advanced_monitoring/blob/master/neural-network/classification/img/mat.png?raw=true"/>

### Input Layer:

<img src="https://github.com/vinhali/advanced_monitoring/blob/master/neural-network/classification/img/input_layer.png?raw=true"/>

### Output Layer:

<img src="https://github.com/vinhali/advanced_monitoring/blob/master/neural-network/classification/img/output_layer.png?raw=true"/>

### Normalize:

                 m1,d1           m2,d2           m3,d3           m4,d4           m5,d5   0-20  20-40  40-60  60-80  80-100
    0   [0.573, 0.699]  [0.412, 0.224]  [0.696, 0.512]  [0.326, 0.314]   [0.79, 0.685]  1.000    0.5      0      0       0
    1   [0.456, 0.251]  [0.629, 0.523]  [0.344, 0.286]    [0.8, 0.699]    [0.721, 1.0]  1.000    0.5      0      0       0
    2   [0.658, 0.531]  [0.339, 0.282]  [0.592, 0.614]    [0.859, 1.0]  [0.365, 0.283]  1.000    0.5      0      0       0
    3   [0.396, 0.314]   [0.29, 0.201]      [1.0, 1.0]   [0.34, 0.288]  [0.886, 0.647]  1.000    0.5      0      0       0
    4   [0.379, 0.315]      [1.0, 1.0]  [0.302, 0.248]  [0.929, 0.655]  [0.328, 0.308]  1.000    0.5      0      0       0
    5       [1.0, 1.0]  [0.274, 0.249]  [0.679, 0.536]   [0.52, 0.413]  [0.382, 0.337]  1.000    0.5      0      0       0
    6   [0.657, 0.898]  [0.324, 0.244]  [0.796, 0.565]  [0.336, 0.337]   [0.594, 0.48]  1.000    0.5      0      0       0
    7   [0.397, 0.312]  [0.731, 0.583]  [0.358, 0.296]  [0.586, 0.495]  [0.263, 0.228]  1.000    0.5      0      0       0
    8   [0.792, 0.589]  [0.343, 0.296]  [0.472, 0.434]  [0.278, 0.227]  [0.432, 0.316]  1.000    0.5      0      0       0
    9   [0.395, 0.366]  [0.301, 0.247]  [0.449, 0.413]  [0.438, 0.324]  [0.798, 0.545]  1.000    0.5      0      0       0
    10  [0.411, 0.347]  [0.436, 0.417]  [0.373, 0.276]   [0.78, 0.565]  [0.399, 0.274]  1.000    0.5      0      0       0
    11  [0.521, 0.443]  [0.221, 0.206]  [0.658, 0.476]   [0.512, 0.35]  [0.364, 0.264]  1.000    0.5      0      0       0
    12  [0.277, 0.271]  [0.364, 0.264]  [0.729, 0.477]  [0.325, 0.265]  [0.544, 0.447]  0.964    1.0      0      0       0
    13  [0.409, 0.312]  [0.693, 0.484]  [0.333, 0.234]  [0.551, 0.458]  [0.297, 0.246]  0.964    1.0      0      0       0
    14  [0.728, 0.487]  [0.332, 0.233]  [0.439, 0.403]  [0.309, 0.245]  [0.327, 0.231]  1.000    0.5      0      0       0
    15  [0.368, 0.287]  [0.279, 0.229]   [0.46, 0.396]  [0.309, 0.235]  [0.854, 0.675]  1.000    0.5      0      0       0
    16  [0.342, 0.283]  [0.454, 0.397]  [0.268, 0.203]  [0.888, 0.685]  [0.208, 0.117]  1.000    0.5      0      0       0
    17  [0.492, 0.422]  [0.238, 0.201]  [0.676, 0.585]  [0.329, 0.233]   [0.24, 0.147]  1.000    0.5      0      0       0
    18  [0.311, 0.272]  [0.389, 0.468]  [0.567, 0.433]   [0.21, 0.145]  [0.561, 0.435]  1.000    0.5      0      0       0
    19  [0.456, 0.485]  [0.518, 0.447]  [0.223, 0.131]  [0.552, 0.447]  [0.254, 0.185]  1.000    0.5      0      0       0
    20  [0.689, 0.597]   [0.23, 0.126]  [0.386, 0.392]    [0.363, 0.2]  [0.276, 0.211]  1.000    0.5      0      0       0
    21  [0.202, 0.129]  [0.228, 0.158]  [0.503, 0.376]  [0.258, 0.215]  [0.813, 0.551]  1.000    0.5      0      0       0
    22  [0.248, 0.173]  [0.493, 0.379]  [0.232, 0.187]  [0.804, 0.571]  [0.295, 0.223]  1.000    0.5      0      0       0
    23  [0.484, 0.392]   [0.216, 0.18]   [0.583, 0.46]   [0.46, 0.342]  [0.281, 0.226]  1.000    0.5      0      0       0
    24  [0.255, 0.215]  [0.311, 0.288]  [0.658, 0.444]  [0.248, 0.227]  [0.582, 0.523]  1.000    0.5      0      0       0
    25  [0.357, 0.316]  [0.619, 0.456]    [0.258, 0.2]   [0.562, 0.54]   [0.29, 0.235]  1.000    0.5      0      0       0
    26    [0.71, 0.49]  [0.246, 0.199]   [0.45, 0.473]  [0.314, 0.237]  [0.343, 0.235]  1.000    0.5      0      0       0
    27  [0.284, 0.247]  [0.256, 0.227]  [0.484, 0.458]  [0.308, 0.239]   [0.842, 0.54]  1.000    0.5      0      0       0
    28  [0.316, 0.272]   [0.476, 0.46]  [0.268, 0.206]   [0.835, 0.56]  [0.296, 0.168]  1.000    0.5      0      0       0
    29  [0.529, 0.483]  [0.225, 0.202]   [0.602, 0.45]  [0.433, 0.314]  [0.291, 0.187]  1.000    0.5      0      0       0
    30  [0.288, 0.273]  [0.324, 0.247]   [0.647, 0.45]  [0.256, 0.182]  [0.743, 0.572]  1.000    0.5      0      0       0
    31   [0.37, 0.292]  [0.619, 0.459]   [0.268, 0.16]  [0.589, 0.532]  [0.569, 0.341]  1.000    0.5      0      0       0
    32  [0.688, 0.479]  [0.266, 0.158]   [0.451, 0.47]  [0.579, 0.344]  [0.426, 0.234]  1.000    0.5      0      0       0
    33  [0.259, 0.174]  [0.237, 0.159]  [0.696, 0.495]  [0.421, 0.231]  [0.855, 0.551]  1.000    0.5      0      0       0
    34  [0.261, 0.193]   [0.67, 0.502]  [0.375, 0.197]  [0.714, 0.511]  [0.566, 0.401]  1.000    0.5      0      0       0
    35  [0.635, 0.509]  [0.394, 0.199]   [0.571, 0.45]   [0.515, 0.39]   [0.421, 0.28]  0.964    1.0      0      0       0
    36  [0.419, 0.231]   [0.36, 0.232]  [0.648, 0.499]   [0.405, 0.28]  [0.551, 0.452]  0.964    1.0      0      0       0
    37   [0.409, 0.28]  [0.659, 0.496]  [0.357, 0.235]  [0.531, 0.464]   [0.308, 0.25]  0.964    1.0      0      0       0
    38    [0.741, 0.5]  [0.343, 0.238]   [0.414, 0.41]  [0.324, 0.247]  [0.531, 0.438]  1.000    0.5      0      0       0
    39  [0.393, 0.291]  [0.249, 0.213]  [0.468, 0.403]  [0.336, 0.256]    [1.0, 0.588]  1.000    0.5      0      0       0
    40   [0.322, 0.27]  [0.452, 0.407]   [0.293, 0.22]    [1.0, 0.612]  [0.346, 0.247]  1.000    0.5      0      0       0
    41    [0.5, 0.427]  [0.243, 0.215]  [0.739, 0.508]   [0.479, 0.35]  [0.278, 0.161]  1.000    0.5      0      0       0
    42  [0.307, 0.285]  [0.469, 0.399]  [0.678, 0.449]  [0.248, 0.159]  [0.801, 0.708]  1.000    0.5      0      0       0
    43   [0.51, 0.428]  [0.655, 0.456]  [0.257, 0.142]  [0.804, 0.724]   [0.23, 0.144]  1.000    0.5      0      0       0
    44   [0.726, 0.47]   [0.245, 0.14]  [0.501, 0.618]   [0.44, 0.299]   [0.276, 0.17]  1.000    0.5      0      0       0
    45  [0.288, 0.226]  [0.215, 0.121]   [0.68, 0.625]  [0.249, 0.171]  [0.701, 0.716]  1.000    0.5      0      0       0
    46  [0.229, 0.148]  [0.683, 0.624]  [0.222, 0.146]  [0.687, 0.732]  [0.296, 0.184]  1.000    0.5      0      0       0
    47  [0.646, 0.627]  [0.204, 0.152]  [0.542, 0.632]   [0.36, 0.224]  [0.291, 0.215]  1.000    0.5      0      0       0
    
 ### Explicação da normalização:

> *m1 = Média de 12 leituras (Em uma janela de 60 dados) - Exemplo ((2 + 5 + 7 ...) / 12*

> *d1 = desvio padrão dos 12 dados*

E assim sucessivamente até formar m5, d5 (12x5 = 60)

> *0-20 = Quantas vezes os valores são repetidos no intervalo de 0 a 20 nas próximas 30 leituras (Linha 61,62,62 ...)*

E assim sucessivamente até formar 20-40.40-60.60-80.80-100


